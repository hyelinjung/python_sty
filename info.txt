#Streamlit 이란
->파이썬으로 데이터 애플리케이션을 빠르고 간편하게 개발할 수 있도록 도화주는 오픈 소스 프레임워크.
->최소한의 코드로 대화형 웹 애플리케이션을 만들고 배포할 수 있게 해줌

#R2와 MSE는 언제 사용하는가?
-R2:
모델이 종속 변수의 변동성을 얼마나 잘 설명하는지를 알고 싶을때 사용
주로 모델 간 상대적인 비교에 유용하다.예를 들어 두개의 회귀 모델 중 어떤 것이 더 데이터를 잘 설명하는지 비교할 때 사용
-MSE:
모델이 실제값과 얼마나 가까운지를 절대적인 수치로 측정할 때 사용한다.
예측의 정확도를 평가할 때 유용하며 측정단위(주가,온도 등)에 대한 오차 크기를 직접 확인할 수 있다

#로지스틱 회귀가 무엇이고 언제 사용하는가
-무엇)입력 데이터로부터 0또는 1같은 이진 결과를 예측하는것으로 사실상 분류 알고리즘으로 사용됨
(이멜이 스팸인지 아닌지/ 제품을 구매할지 말지 예측할 때)
사용이유) 로지스틱 회귀는 연속적인 입력 변수를 받아 이진 출력을 얘측하는데 적합/ 출력값으로 확률을 반환
-언제사용)환자의 나이,혈압 등을 입력받아 특정 질병에 걸릴 확률을 예측

#랜덤 포레스트는 무엇이고 언제 사용하는가
-무엇)여러개의 결정트리를 생성한 뒤, 그 결과를 종합하여 더 좋은 예측 성능을 내는 알고리즘.
분류와 회귀 문제 모두에 사용할 수 있다.
-어떤 흐름으로 진행)
->전체 데이터에서 중복을 허락해 무작위로 샘플을 뽑아 여려개의 훈련 데이터 집합을 만들고 이것을 각각 결정 트리에 사용한다.
->위의 데이터를 사용해서 피처도 무작위로 일부만 선택해 최적의 분할을 잦는다

(동작방식)
1.원본 데이터를 중복을 허용해 랜덤하게 샘플링해 여러 개의 그룹을 만듫
2.각각의 그룹에 대한 나무를 생성하고 이때 각 노드를 분할할때 사용할 피처도 랜덤하게 선택함
3.각 나무의 예측결과를 회귀또는 분류로 결합해 최종 예측을 만든다.

->  [분류문제에서는] 각 트리의 예측 결과 중 가장 많은 표를 얻은 클래스를 최종 예측값으로 사용
    [회귀] 각 트리의 예측값 평균을 최종 예측값으로 사용한다.
-언제사용) 비선형 데이터를 가진 복잡한 데이터

※ 데이터 전처리 : 모델 학습의 성능과 정확성을 높이기 위해 row 데이터를 정리하고 변환하는 
->주요단계)
-결측값 처리(데이터셋에 누락된 값), 결측값이 있는 행이나 열을 삭제 하거나 평균,중앙값등으로 결측값을 채운다
-이상치(데이터 분포에서 일반적인 패턴에서 크게 벗어난 값)을 처리함
-데이터 스케일링(서로 다른 범위를 가진 변수들이 동일한 범위에서 비교될 수 있도록 데이터를 변환한다)
->표준화(평균이0 표준편차가1이 되도록 변환) / 정규화(최소값0 최대값1로 변환)
-범주형(Categorical) 데이터를 수치형(Numerical)으로 변환
->label encoding 범주형 값을 고유 숫자로 변환
->one-Hot Encoding: 범주형 값을 이진 벡터로 변환
※random_state=42
->데이터 분할 시 랜덤성을 제어하는 파라미터
->왜 사용하는 가? 훈련하기 위해 데이터를 무작위로 나누기 때문에, 실행하 때마다 결과가 달라질 수 있다.
결과가 같기 위해선 이를 사용하면 매번 동일한 방식으로 데이터를 나눌 수 있다

※MSE 사용하는 이유
회귀모델에서 예측값과 실제값 사이의 차이를 측정하기 위해 자주 사용하는 손실함수.
->직관적인 해석: MSE는 오차의 제곱 평균을 의미하기 때문에 손실값이 클수록 예측값이 실제값과 더 멀리 떨어져 있다는 말
즉, MSE가 0에 가까우면 모델이 거의 완벽한 예측을 함
※※ MSE vs MAE 
-> 전자)큰 오차에 더 큰 페널티를 주기 때문에 큰 오차를 줄이는데 효과적인. 후자)각 오차의 절대값을 평균 내기 때문에 이상치에 덜 민감하며, 전반적인 오차의 크기를 평가하는데 적함

※matplotlib.pyplot.plot() 함수
->선 크래프를 그리기 위한 함수. y=x 대각선을 그려 완벽한 예측을 시각적으로 나타낸다.
y=x선 
'같' 예측값과 실제값이 정확하게 일치
'아래' 모델이 실제값보다 낮은 값을 예측
'위' 실제값보다 높은 값 예측

머신러닝 : 데이터를 기반으로 패턴을 인식하는 기술은 맞지만 새로운 데이터를 만들어가는 것
이라고 설명하기보다 기존 데이터를 바탕으로 예측,분류,회귀 등 특정 작업을 수행하는 모델을 학습하는 것.
딥러닝 : 머신러닝의 하위 분야...복잡한 데이터에서 더 깊은 패턴을 추출


※새로운 데이터로 예측 -공부6시간 과외2시간
print(f'\n예측 점수:{prediction[0]:.1f}')

->인덱스 첫 번째 요소를 가져온 후 소수점 1자리로 반올림하여 실수 형식으로 출력 ex)3.456 ->3.5 출력

※회귀 계수(coef_) : 선형 회귀 모델에서 각 독립 변수에 대한 기울기를 나타냄 즉,
 독립 변수가 종속 변수에 미치는 영향을 나타낸다.
->model.coef_[0]이라는 말은 model.coef_[0]가 [1.0,1.0]이라면 1.0이된다. 즉
첫 번째 특성(예를 들어 x의 첫 번째 열이 1,2,3,4일때) 대한 회귀 계수가 1.0임을 말한다.
=> 첫 번째 측성의 값이 1증가할때마다 y의 값도 1증가
-배열로 나타나는 값은 피쳐의 개수만큼 나타나는가?
=피처가 5개라면 coef_는 길이가 5인 배열이 되고 각 값은 해당 피처가 타깃 변수에 미치는 영향을 의미

※회귀 모델의 기본 형태 y= a*X +b

※X_train, X_test, y_train, y_test 
= train_test_split(X,y,test_size=0.2,random_state=42)처럼 
train_test_split을 사용해 훈련셋과 테스트 셋을 생성하는 이유

->주 이유는 모델의 성능을 파악하고 과적합을 방지하기 위해서 데이터를 나눈다.
->훈련데이터로만 모델을 평가하면 '과적합'의 위험이 있다..즉 모델이 훈련 데이터에 지나티게 적합하여 새로운 데이터에 대한 예측 성능이 떨어지는 현상
X : features데이터, 즉 독립변수들이 담긴 데이터셋
y : target 데이터, 즉 종속 변수
->x와y를 랜덤하게 분할해 비율0.2에 맞게 훈련데이터와 테스트데이터를 나누고 이결과를 4개의 변수에 할당
X_train : 훈련 데이터에 해당하는 특성 값들
y_train : 훈련 데이터에 대한 목표 값
X_test : 테스트 데이터에 해당하는 특성값
y_test : 테스트 데이터에 대한 목표 값

※ model.predict([5]) 실행 시 "ValueError: Expected 2D array, got 1D array instead: array=[5]"발생
->model.fit()은 인자로 2D배열을 기대한다.
->model.predict([5]) VS model.predict([[5]])
후자는 2D배열로 1개의 샘플,1개의 특성(5)을 가진 데이터 / 전자는 샘플 개수 없이 하나의 특성값만 가진 1D배열

※model.fit()인자에 들어가는 훈련데이터
->해당 메서드를 사용해 훈련데이터로 모델의 파라미터(가중치,계수)를 조정
->모델은 훈련데이털르 기반으로 학습해 패턴이나 관계를 가지고 이를 기반으로 예측을 할수 있도록 한다.
(흐름)모델은 X_train에 특성과 y_train에서 제공된 목표값을 비교하면서 학습... 이 과정에서 모델은 파라미터를 조정,훈련 데이터에서 나타나는 관계를
잘 표현할 수 있게 한다.

※sns.barplot()에서 x축,y축을 독립,종속 변수라고 생각
->그 와는 별개로, 데이터 시각화를 위한 축으로 사용된다. x축과 y축 값은 그래프를 그리는 목적에 따라 지정된다.
x축 :그래프에서 수치형 데이터 ,특성의 중요도 같은 연속적인 값을 나타냄
y축 :범주형 데이터 또는 특성 이름과 같은 순서가 없는 항목을 나타냄

※classifier.predict([[6]]) 처럼 모델로 예측을 할 때 인자값 의미
->데이터셋의 인덱스가 아니라,예측하고자 하는 데이터 포인트의 특성 값,
즉 해당 모델에 값 6을 가진 새로운 데이터 포인트를 입력해 그에 대한 예측값을 반환
->결과값 array([0], dtype=int64)

※TfidfVectorizer의 fit_transform()
->벡터화 작업(텍스트 데이터를 수치 데이터로 변환)과 TF-IDF 가중치 계산이 함께 이뤄지는 과정
[동작의 흐름]
1.텍스트를 토큰화 -> 텍스트를 단어 단위로 분리
2.모든 텍스트 데이터에서 고유한 단어(특징)을 추출하고 ,각 단어에 고유한 인덱스를 부여한다
3.각 문서에 대해 단어의 빈도(TF)를 계산하고, 단어의 중요도를 조정하기 위해 IDF값을 곱한다
->단순히 단어의 빈도수를 확인하기 위해서가 아니라, 텍스트 데이터를 백터화하고 각 단어의 중요도를 계산하기 위해서
->왜? 학습모델이 사용할수 있는 텍스트가 아니라 수치 데이터로 변환해야 해서


※경사하강법 : 주로 머신러닝 모델의 손실함수를 최소화하기 위해 사용된다
즉,모델이 예측한 값 y, 실제값y_ 간의 차이(손실)를 최소화하는 최적의 파라미터를 찾는 것

※절편 model.intercept
-> 선형 회귀 모델에서 절편은 회귀 직선이 y축과 만나는 점을 의미한다.
->절편은 모든 피쳐 값들이 모두 0일때 모델이 예측하는 출력값. 즉 예측값의 기본 수준을 나타낸다.

※r2_score()와 LinearRegression.score()의 차이
전자)실제값과 예측값을 구해서 입력. 후자)입력데이터(X_test)와 실제값을 제공하면 R2값을 제공


#랜덤포레스트에서는 coef_를 사용하는 대신 feature_importances_를 사용한다 .. 왜?
->해당 모델은 회귀 모델이지만 트리 기반 모델이기 때문에 각 트리로 데이터를 분할해 트리 분할 규칙에 따라 예측하기 때문에,
선형 모델처럼 회귀계수coef_를 사용할 수 없다.

#분류라는 것은 어떠한 것을 구분하는 것이라고 생각하고 있다 -> 어떤 대상을 주어진 클래스중 하나로 구분하는 것과 같은 말
->(개념적) 분류는 입력받은 데이터를 미리 정의된 여러 클래스 중 하나에 속하도록 '예측'하는 것이 목적`
<예시>
-이진 -> 두 개의 클래스(0,1)중 하나로 구분하는
-다중 -> 세 개 이상의 클래스 중 하나로 구분(정치,경제,스포츠등 여러 주제 중  하나로 분류)

#범주형 데이터 인코딩
-레이블 인코딩/원-핫 인코딩/카운드 인코딩
